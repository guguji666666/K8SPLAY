## ğŸ¯ Why Pod Cleaner?

| Pain Point | What Pod Cleaner Does |
|---|---|
| CrashLoopBackOff keeps restarting but never recovers | Detects + deletes the pod to trigger a clean recreation |
| Pod phase shows `Running` but container is actually crashed | Checks **pod phase + container state** (waiting/terminated) |
| Manual monitoring is required | Runs automatically on an interval (default **10 minutes**) |
| No visibility into pod issues | Sends Bark alerts with reason/message/restart count |
| Testing requires a real cluster | Provides local test scripts to validate detection logic |
| Notification config gets hardcoded | Uses env vars (`BARK_BASE_URL`, `BARK_ENABLED`) |
| Large clusters (10k+ pods) are slow to scan | Uses pagination (`limit=500`) to reduce API pressure |

---

## âœ¨ Features

### Core
- âœ… Periodic execution (default every **600s / 10min**)
- âœ… Detect unhealthy pods beyond phase checks
- âœ… Restart unhealthy pods by deletion (controller will recreate)
- âœ… Logs every detection + cleanup action
- âœ… Optimized for large clusters with pagination

### Bonus
- ğŸ›ï¸ Bark push notifications with detailed context
- âœ… Recovery verification with polling (cluster-size aware)

---

## ğŸ§  Detection Logic & Recovery Verification

### Pod Health Detection

Pod Cleaner focuses on pods that appear "normal" at a glance but are actually broken:

- Pod in `Running`/`Init` **but** container state is:
  - `waiting` with reasons like `CrashLoopBackOff`, `ImagePullBackOff`, etc.
  - `terminated` with non-zero exit code

### Recovery Verification (Bonus)

After restarting unhealthy pods, the tool verifies recovery with intelligent polling:

- **Polling with early exit**: Stops checking once all pods recover
- **Cluster-size awareness**:
  - SMALL (â‰¤50 namespaces): 180s budget, 30s interval
  - MEDIUM (â‰¤200 namespaces): 150s budget, 30s interval
  - LARGE (>200 namespaces): 120s budget, 60s interval
- **Budget protection**: Maximum verification time prevents schedule overruns

> This avoids the common trap: **phase == Running** doesn't mean the container is healthy.

---

## ğŸš« Exclusion Rules

- Skip namespace: `kube-system`
- Only evaluate pods in: `Running` and `Init` (as per current design)

---

## âš™ï¸ Configuration

### Environment Variables

| Variable | Required | Default | Description |
|---|---:|---|---|
| `BARK_BASE_URL` | âœ… Yes (if Bark enabled) | - | Bark push base URL with device key |
| `BARK_ENABLED` | No | `true` | Enable/disable Bark notifications |
| `LOG_LEVEL` | No | `INFO` | `DEBUG/INFO/WARNING/ERROR` |
| `RUN_INTERVAL_SECONDS` | No | `600` | Interval between runs in seconds |

---

## ğŸ›ï¸ Bark Notifications (Optional)

### [Bark github](https://github.com/Finb/bark-server)
### [Bark request methods](https://bark.day.app/#/en-us/tutorial?id=request-methods)

Pod Cleaner can send push notifications via Bark.

### Use Public Bark

Install **Bark** from the iOS App Store, get your device key, then set:

```bash
export BARK_BASE_URL="https://api.day.app/YOUR_DEVICE_KEY"
export BARK_ENABLED="true"
```

### Self-host Bark Server (Docker Compose) for quick test

```yaml
version: "3.8"
services:
  bark-server:
    image: finab/bark-server
    container_name: bark-server
    restart: always
    volumes:
      - ./data:/data
    ports:
      - "8080:8080"
```

```bash
docker compose up -d
# then open: http://localhost:8080 to get your device key
```

### Self-host Bark Server (Kubernetes) for production

For production deployments, deploy Bark server to Kubernetes using the provided manifest:

```bash
# Deploy Bark server to Kubernetes
kubectl apply -f bark-server.yaml

# Check deployment status
kubectl get deployment bark-server
kubectl get svc bark-server

# Get external IP or NodePort to access the Bark UI
kubectl get svc bark-server -o jsonpath='{.spec.ports[0].nodePort}'
```

**Expected output:**
```
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
bark-server    1/1     1            1           30s

NAME           TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
bark-server    NodePort   10.96.XXX.XXX   <none>        8080:30080/TCP   30s
```

**Access Bark UI:**
- If using NodePort: `http://<node-ip>:30080`
- If using LoadBalancer: `http://<loadbalancer-ip>:8080`

**Get device key:**
1. Open Bark UI in browser
2. Register device to get your device key
3. Use the key in `BARK_BASE_URL`

**Cleanup:**
```bash
kubectl delete -f bark-server.yaml
```

### Quick test

```bash
curl -X POST "${BARK_BASE_URL}" \
  -H "Content-Type: application/json" \
  -d '{"title":"Test","body":"Pod Cleaner test notification"}'
```

---

## ğŸ§ª Testing

### 1) `test-detection-logic.py` â€” Detection Logic Validator

Validates pod health detection logic without a real cluster (and optionally against a real cluster).

```bash
# Run logic tests only
python3 test-detection-logic.py

# Check real cluster
python3 test-detection-logic.py --k8s

# Check a namespace
python3 test-detection-logic.py --k8s -n test-failures
```

screenshot for `python3 test-detection-logic.py --k8s` <br>
<img width="1158" height="486" alt="image" src="https://github.com/user-attachments/assets/f760f79c-8a36-4609-bc00-9aeec9916825" />

Covered cases:

* CrashLoopBackOff (waiting)
* ImagePullBackOff (waiting)
* Abnormal terminated (exitCode != 0)
* Normal running / normal terminated (exitCode 0)


screenshot for Step 4: Send cleanup notification after restarting unhealthy pods in main.py <br>
<img width="1206" height="2622" alt="image" src="https://github.com/user-attachments/assets/3c8509fd-2a6e-4224-afc0-9f7623b8ebc4" />

screenshot for Step 5: trigger notifications if some pods still failed to start  in main.py <br>
<img width="1206" height="2622" alt="image" src="https://github.com/user-attachments/assets/f04a327e-3254-4da8-a6d1-af02485acccb" />


### 2) `100-namespace.yaml` â€” Cluster Scale Simulation

Creates 100 test namespaces (`test-001` to `test-100`) to simulate cluster scale testing. Each namespace contains a simple nginx deployment.

**Purpose:** Test pod-cleaner performance and behavior under large-scale cluster conditions.

```bash
# Deploy 100 namespaces with nginx
kubectl apply -f 100-namespace.yaml

# Verify namespaces created
kubectl get ns | grep test- | head -20
# Expected: test-001, test-002, ... test-100

# Check pod distribution
kubectl get pods -n test-001  # Should show nginx deployment

# Cleanup
kubectl delete -f 100-namespace.yaml
```

**Expected observations:**

| Metric | Expected Value |
|--------|---------------|
| Namespace count | 100 (test-001 to test-100) |
| Pods per namespace | 1 nginx pod |
| Total pods | 100 nginx pods |
| Detection time | ~30-60s (depends on cluster size) |

---

### 3) `test-failure-pods.yaml` â€” Failure Simulation Pods

Creates 6 types of failing pods for testing pod-cleaner detection:

| Type | Failure Pattern | Pod Phase | Container State |
|------|----------------|-----------|-----------------|
| CrashLoopBackOff | Exits immediately, restarts repeatedly | Running | waiting (CrashLoopBackOff) |
| ErrorExit | Runs 30s then exits with code 1 | Running | terminated (exitCode=1) |
| InitFailure | Init container fails | Init:Error | N/A |
| Mixed | One healthy + one failing container | Running | Mixed states |
| ImagePullBackOff | Non-existent image | Pending | N/A |
| OOMKilled | Memory limit exceeded | Running | OOMKilled |

```bash
kubectl apply -f test-failure-pods.yaml
kubectl get pods -n test-failures -w
```

Expected observations:

| Pod Type | Expected Status | Restart Behavior |
|----------|----------------|-----------------|
| crashloop-pod | CrashLoopBackOff | RESTARTS increasing |
| error-exit-pod | Running â†’ Error | RESTARTS 0â†’1+ |
| init-failure-pod | Init:Error | RESTARTS 1+ |
| mixed-pod | Running | One container restarting |
| image-pull-fail-pod | Pending/ImagePullBackOff | No restarts |
| oom-pod | Error/OOMKilled | RESTARTS 1+ |

---

## â˜¸ï¸ Kubernetes Deployment

### Method A: Helm (Recommended)

if you build your own docker image
```bash
helm install pod-cleaner ./helm/pod-cleaner \
  --set image.repository=<your repo> \
  --set image.tag=<your image tag> \
  --set config.barkBaseUrl="https://your-bark-server.com/DEVICE_KEY" \
  --set config.barkEnabled=true \
  --set config.logLevel="INFO"

kubectl get deployment pod-cleaner
kubectl logs -l app=pod-cleaner -f
```

if you want to use existing docker image
```bash
helm install pod-cleaner ./helm/pod-cleaner \
  --set image.repository=guguji666/pod-cleaner \
  --set image.tag=latest \
  --set config.barkBaseUrl="https://your-bark-server.com/DEVICE_KEY" \
  --set config.barkEnabled=true \
  --set config.logLevel="INFO"

kubectl get deployment pod-cleaner
kubectl logs -l app=pod-cleaner -f
```

if you want to check the logs of pod-cleaner lively
```bash
kubectl get pods
kubectl logs -f <pod id>
```
<img width="2934" height="1892" alt="image" src="https://github.com/user-attachments/assets/c437ce4c-aa28-42a5-aa1a-0574e036878b" />


### Method B: Native Manifest

```bash
# edit image in k8s-manifest.yaml first
kubectl apply -f k8s-manifest.yaml

kubectl get pods -l app=pod-cleaner
kubectl logs -l app=pod-cleaner -f
```

---

## ğŸ—ï¸ Architecture & Implementation


### Runtime Dependencies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Runtime Dependencies                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚   src/main.py       â”‚  â† Entry point
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â”‚ imports
                                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                       â”‚                       â”‚
              â–¼                       â–¼                       â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  src/config.py  â”‚   â”‚ kube_client.py   â”‚   â”‚  src/notifier.py â”‚
    â”‚  (Configuration)â”‚   â”‚  (K8s Client)   â”‚   â”‚  (Notifications)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                       â”‚                       â”‚
              â”‚                       â”‚                       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Kubernetes API       â”‚
                        â”‚   Bark Server          â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Build & Deployment Dependencies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Build & Deployment Flow                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

requirements.txt â”€â”€â”€â”€â”€â”€â–º Dockerfile â”€â”€â”€â”€â”€â”€â–º pod-cleaner:latest (Image)
      â”‚                      â”‚
      â”‚                      â”‚ references
      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Python deps     â”‚   â”‚ Image build     â”‚
â”‚ - kubernetes    â”‚   â”‚ - Base image   â”‚
â”‚ - requests      â”‚   â”‚ - Install deps  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ - Copy src/     â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Push to registry       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                â”‚                â”‚
              â–¼                â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ k8s-manifest.yamlâ”‚ â”‚ Helm values.yamlâ”‚ â”‚ Helm Chart.yaml â”‚
    â”‚ (Native K8s)    â”‚ â”‚  (Helm params)  â”‚ â”‚  (Chart meta)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                   â”‚
             â”‚ references        â”‚ references
             â–¼                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Helm templates/  â”‚ â”‚ Image URL at deployment â”‚
    â”‚ - deployment.yaml â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ - rbac.yaml     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

README.md (Documentation)
```

### Core Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Runtime Data Flow                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1ï¸âƒ£  STARTUP
    main.py â†’ config.py â†’ Read configuration (Bark URL, interval, etc.)

2ï¸âƒ£  CONNECT TO CLUSTER
    main.py â†’ kube_client.py â†’ Kubernetes API
                                 â”‚
                                 â–¼
                         Get all namespaces

3ï¸âƒ£  FIND UNHEALTHY PODS
    kube_client.py â†’ Iterate each namespace
                      â†’ Check each pod's phase
                      â†’ Filter out non-Running/Init pods

4ï¸âƒ£  CLEANUP PODS
    kube_client.py â†’ delete_namespaced_pod() â†’ Delete unhealthy pods

5ï¸âƒ£  SEND NOTIFICATIONS
    main.py â†’ notifier.py â†’ Bark Server
                                  â”‚
                                  â–¼
                          Push notification to phone

6ï¸âƒ£  CHECK RECOVERY (Bonus)
    kube_client â†’ wait_for_pods_ready() â†’ Wait and recheck
                  â†’ If still unhealthy â†’ notifier.send_alert()
```

### File Function Quick Reference

| File | Responsibility | Dependencies |
|------|----------------|--------------|
| `src/main.py` | Main loop, orchestration | Entry point (no dependencies) |
| `src/config.py` | Configuration parameters | main, kube_client, notifier |
| `kube_client.py` | Kubernetes API operations | main |
| `src/notifier.py` | Bark push notifications | main |
| `Dockerfile` | Container image build | CI/CD |
| `k8s-manifest.yaml` | Native K8s deployment | kubectl |
| `helm/*.yaml` | Helm deployment | helm |
| `README.md` | Documentation | Developers |


### Main Loop Flow Chart

```
main.py
â”‚
â”œâ”€â”€ main()                                # Program entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ setup_logging()                  # Initialize logging system
â”‚   â”‚
â”‚   â”œâ”€â”€ KubernetesClient()               # Initialize K8s API client
â”‚   â”‚   â””â”€â”€ load_incluster_config()      # Use Pod ServiceAccount for auth
â”‚   â”‚
â”‚   â”œâ”€â”€ BarkNotifier()                   # Initialize notification module
â”‚   â”‚   â””â”€â”€ Config.get_bark_base_url()   # Read Bark push URL from config
â”‚   â”‚
â”‚   â””â”€â”€ while True:                      # Main daemon loop
â”‚       â”‚
â”‚       â”œâ”€â”€ get_all_namespaces()          # Fetch all cluster namespaces
â”‚       â”‚
â”‚       â”œâ”€â”€ find_unhealthy_pods()       # Core detection logic
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€â”€ should_skip_namespace()   # Exclude system namespaces (kube-system)
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€â”€ is_pod_healthy()        # Pod phase screening (Running/Init/Succeeded)
â”‚       â”‚   â”‚
â”‚       â”‚   â””â”€â”€ check container state    # Container real status check
â”‚       â”‚       â”œâ”€â”€ waiting              # CrashLoopBackOff / ImagePullBackOff
â”‚       â”‚       â””â”€â”€ terminated(exit!=0)  # Abnormal exit with non-zero code
â”‚       â”‚
â”‚       â”œâ”€â”€ restart_pods()                # Batch self-healing (delete pods)
â”‚       â”‚   â””â”€â”€ delete_pod()            # Call K8s API to delete single pod
â”‚       â”‚
â”‚       â”œâ”€â”€ send_cleanup_report()        # Send cleanup execution report
â”‚       â”‚
â”‚       â”œâ”€â”€ wait_for_pods_ready()       # Wait + recovery confirmation (Bonus)
â”‚       â”‚
â”‚       â””â”€â”€ send_alert()                 # Alert if pods still unhealthy
â”‚
â””â”€â”€ sleep(RUN_INTERVAL_SECONDS)           # Wait before next cycle
```

### Key Processing Steps

| Step | Function | Purpose |
|------|----------|---------|
| 1 | `get_all_namespaces()` | List all namespaces to inspect |
| 2 | `find_unhealthy_pods()` | Detect pods needing restart |
| 3 | `restart_pods()` | Delete unhealthy pods (trigger ReplicaSet recreation) |
| 4 | `send_cleanup_report()` | Notify cleanup summary |
| 5 | `wait_for_pods_ready()` | Verify recovery with polling |
| 6 | `send_alert()` | Alert if still unhealthy |
| 7 | `sleep()` | Maintain 10-minute cadence |



### Implementation Details

| Feature | Location | Key Code |
|---------|----------|-----------|
| **Pagination** | `src/kube_client.py:88-117` | `list_namespaced_pod(limit=500)` |
| **Skip kube-system** | `src/config.py:29` | `EXCLUDED_NAMESPACES = ["kube-system"]` |
| **Idempotency** | `src/main.py:76-127` | Re-check status every cycle |
| **RBAC Least Privilege** | `k8s-manifest.yaml:23-35` | `verbs: ["get","list","delete"]` |
| **ServiceAccount** | `k8s-manifest.yaml:8-15` | `serviceAccountName: pod-cleaner-sa` |
| **NonRoot** | `k8s-manifest.yaml:87-90` | `runAsUser: 1000`, `runAsNonRoot: true` |
| **Graceful Restart** | `src/kube_client.py:256-259` | `grace_period_seconds=0` |
| **10min Interval** | `src/config.py:41` | `RUN_INTERVAL_SECONDS = 600` |

### Code Highlights

#### 1) Pagination (kube_client.py)

```python
# Each API call returns max 500 pods
# Loop continues while _continue token exists
while True:
    resp = self.api.list_namespaced_pod(
        namespace=namespace,
        limit=500,
        _continue=continue_token  #ä¸‹ä¸€é¡µçš„å‡­è¯
    )
    all_pods.extend(resp.items)
    continue_token = resp.metadata._continue
    if not continue_token:
        break  # No more pages
```

#### 2) Skip kube-system (config.py)

```python
class Config:
    EXCLUDED_NAMESPACES = ["kube-system"]  # Don't touch system pods!

def should_skip_namespace(namespace: str) -> bool:
    return namespace in Config.EXCLUDED_NAMESPACES
```

#### 3) Restart with RBAC (kube_client.py)

```python
def delete_pod(self, namespace: str, pod_name: str) -> bool:
    # RBAC: requires "delete" permission on "pods" resource
    self.api.delete_namespaced_pod(
        name=pod_name,
        namespace=namespace,
        grace_period_seconds=0  # Immediate restart
    )
```

#### 4) 10-Minute Scheduler (main.py)

```python
# Calculate sleep to maintain 10-minute cadence
elapsed = (datetime.now() - run_start_time).total_seconds()
sleep_time = max(0, Config.RUN_INTERVAL_SECONDS - elapsed)
time.sleep(sleep_time)
```

---

## ğŸ§© Design points

### 1) Large-scale performance

* Pagination (`limit=500`) to reduce API load
* Skip `kube-system`
* Optional: label selector narrowing
* Delete pods one-by-one (lower burst load on apiserver)
* Recovery verification with polling (early exit on success)
* Cluster-size aware verification intervals

### 2) Idempotency / Safety

* Deleting a pod is idempotent for ReplicaSet-managed workloads
* Recovery verification protects against false positives
* Configurable verification budget per cluster size
* Dry-run mode support (future)

### 3) Security

* RBAC least privilege (pods: get, list, delete)
* Namespace exclusions to avoid system impact
* Non-root container execution

### 4) Observability

* Structured logging for easy parsing
* Bark notifications for critical alerts
* Recovery verification status in logs

---

## ğŸ“ Project Structure

```text
pod-cleaner/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ kube_client.py
â”‚   â””â”€â”€ notifier.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ k8s-manifest.yaml
â”œâ”€â”€ helm/pod-cleaner/
â”‚   â”œâ”€â”€ Chart.yaml
â”‚   â”œâ”€â”€ values.yaml
â”‚   â””â”€â”€ templates/
â”‚       â”œâ”€â”€ deployment.yaml
â”‚       â”œâ”€â”€ rbac.yaml
â”‚       â””â”€â”€ _helpers.tpl
â”œâ”€â”€ test-detection-logic.py
â”œâ”€â”€ test-failure-pods.yaml
â””â”€â”€ README.md
```

---

## ğŸ³ Docker Build & Run (Single Instance Testing)

**Goal:** run Pod Cleaner in Docker without Helm/K8s deployment.

### Option A: Build & Run

```bash
docker build -t pod-cleaner:latest .

docker run -d \
  --name pod-cleaner \
  -e BARK_BASE_URL="https://your-bark-server.com/DEVICE_KEY" \
  -e BARK_ENABLED="true" \
  -e LOG_LEVEL="INFO" \
  -v ~/.kube/config:/root/.kube/config:ro \
  pod-cleaner:latest

docker logs -f pod-cleaner
```

### Option B: Pre-built Image

```bash
docker run -d \
  --name pod-cleaner \
  -e BARK_BASE_URL="https://your-bark-server.com/DEVICE_KEY" \
  -e BARK_ENABLED="true" \
  -v ~/.kube/config:/root/.kube/config:ro \
  guguji666/pod-cleaner:latest
```

**Windows kubeconfig mount example**

```bash
docker run -d ^
  --name pod-cleaner ^
  -e BARK_BASE_URL="https://your-bark-server.com/DEVICE_KEY" ^
  -e BARK_ENABLED="true" ^
  -v C:\Users\YOUR_USER\.kube\config:/root/.kube/config:ro ^
  guguji666/pod-cleaner:latest
```

---

## ğŸ³ Docker Build Files

```
Files used during Docker build:
â”œâ”€â”€ requirements.txt   â†’ Installed via pip
â”œâ”€â”€ Dockerfile         â†’ Build instructions
â””â”€â”€ src/              â†’ Copied into image
    â”œâ”€â”€ main.py
    â”œâ”€â”€ config.py
    â”œâ”€â”€ kube_client.py
    â””â”€â”€ notifier.py

Files NOT in image:
â”œâ”€â”€ k8s-manifest.yaml  â†’ kubectl apply -f (not in container)
â”œâ”€â”€ helm/              â†’ helm install (not in container)
â”œâ”€â”€ README.md          â†’ Documentation only
â””â”€â”€ test-*.py          â†’ Testing tools only
```

### RBAC Permissions Summary

```yaml
# pod-cleaner-clusterrole (k8s-manifest.yaml)
rules:
# Pod operations
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]     # Read pod status
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["delete"]          # Restart pods

# Namespace read
- apiGroups: [""]
  resources: ["namespaces"]
  verbs: ["get", "list"]

# NOT granted (security):
# - create/update/patch â†’ Can't create new pods
# - exec â†’ Can't enter containers
# - secrets/configmaps â†’ Can't access sensitive data
```

### Key Design Decisions

| Decision | Reason |
|----------|--------|
| Delete instead of restart | K8s has no "restart" API; deleting triggers ReplicaSet recreation |
| grace_period_seconds=0 | CrashLoopBackOff pods won't recover; force immediate restart |
| Skip kube-system | System pods (etcd, API server) must never be deleted |
| Pagination limit=500 | Balances API load vs single-request timeout |
| Cluster-size aware polling | Large clusters need longer budgets, longer intervals |

---


---

## ğŸ—ºï¸ Roadmap for improvement in future 

### Completed âœ…
- âœ… Recovery verification with polling
- âœ… Cluster-size awareness
- âœ… Structured logging

### Planned
- [ ] Slack webhook notifications
- [ ] Email (SMTP) notifications
- [ ] DingTalk webhook
- [ ] Severity-based routing
- [ ] Retry / backoff for notifications
- [ ] Prometheus metrics
- [ ] Health endpoints (`/health`, `/ready`)
- [ ] JSON structured logs
- [ ] Namespace whitelist filter
- [ ] Rate-limited batch operations

---